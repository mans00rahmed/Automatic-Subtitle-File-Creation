{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict the timestamps for possible subtitles of your input audio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from catalyst.dl import SupervisedRunner, CallbackOrder, Callback, CheckpointCallback\n",
    "from fastprogress import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import check_type, extract_audio, mono_load, get_duration\n",
    "from config import InferenceConfig as IC\n",
    "from csrc.configurations import DatasetConfig as DC\n",
    "from csrc.configurations import ModelConfig as MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better debugging\n",
    "\n",
    "os.environ[\"CUDA_LAUCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTANT PARAM\n",
    "PERIOD = 5# 10, 5, 3, 2, 1\n",
    "### IMPORTANT PARAM\n",
    "THRESHOLD = 0.8# 0.5, 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those need no modification.\n",
    "\n",
    "# Audio Sample rate. DO NOT change.\n",
    "SR = DC.dataset_sample_rate\n",
    "\n",
    "# Tag encoding. DO NOT change.\n",
    "CODING = IC.coding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting audio from ./src/data movie/1.mp4\n",
      "MoviePy - Writing audio in src\\data movie\\1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Extraction Successful! Writing 18100482 in src\\data movie\\1.wav.\n",
      "USING MODEL: ./train/logs/sp2-32000hz/checkpoints/best.pth\n"
     ]
    }
   ],
   "source": [
    "# Set and audiohandler your input file.\n",
    "\n",
    "### Target file for inferencing.\n",
    "TARGET_FILE_PATH = \"./src/data movie/1.mp4\"\n",
    "\n",
    "TARGET_FILE_PATH = extract_audio(TARGET_FILE_PATH) if check_type(TARGET_FILE_PATH) else TARGET_FILE_PATH\n",
    "\n",
    "### Output csv file name.\n",
    "OUTPUT_FILE_NAME = \"test\"\n",
    "\n",
    "OUTPUT_FILE = f\"./inf/{OUTPUT_FILE_NAME}.csv\"\n",
    "OUTPUT_SOURCE_FILE = f\"./inf/{OUTPUT_FILE_NAME}-all.csv\"\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    print(\"!Warning: Output file already exists, are you sure to rewrite the file?\")\n",
    "\n",
    "# Target model used for this prediction. Must be corresponding to the model during the training process.\n",
    "\n",
    "### Set the model file path. The file path must be valid\n",
    "MODEL_PATH = \"./train/logs/sp2-32000hz/checkpoints/best.pth\"\n",
    "\n",
    "print(f\"USING MODEL: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csrc.dataset import PANNsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csrc.models import PANNsCNN14Att, AttBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RRSCOM~1\\AppData\\Local\\Temp/ipykernel_11704/890834279.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matt_block\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAttBlock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2048\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matt_block\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    591\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'encoding'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    760\u001b[0m         \u001b[1;31m# only if offset is zero we can attempt the legacy tar file loader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mlegacy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTarError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "model = PANNsCNN14Att(**MC.sed_model_config)\n",
    "model.att_block = AttBlock(2048, 2, activation='sigmoid')\n",
    "model.att_block.init_weights()\n",
    "model.load_state_dict(torch.load(MODEL_PATH)['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, _ = mono_load(TARGET_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_duration\n",
    "audio_duration = get_duration(audio_file_path=TARGET_FILE_PATH, y=y, sr=SR)\n",
    "\n",
    "print(f\"Audio file duration: {audio_duration}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = []\n",
    "\n",
    "len_y = len(y)\n",
    "start = 0\n",
    "end = PERIOD * SR\n",
    "\n",
    "# Split audio into clips.\n",
    "while True:\n",
    "    y_batch = y[start:end].astype(np.float32)\n",
    "    if len(y_batch) != PERIOD * SR:\n",
    "        y_pad = np.zeros(PERIOD * SR, dtype=np.float32)\n",
    "        y_pad[:len(y_batch)] = y_batch\n",
    "        audios.append(y_pad)\n",
    "        break\n",
    "    start = end\n",
    "    end += PERIOD * SR\n",
    "    audios.append(y_batch)\n",
    "\n",
    "# Get tensors\n",
    "arrays = np.asarray(audios)\n",
    "tensors = torch.from_numpy(arrays)\n",
    "\n",
    "estimated_event_list = []\n",
    "global_time = 0.0\n",
    "for image in progress_bar(tensors):\n",
    "    image = image.view(1, image.size(0))\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(image)\n",
    "        framewise_outputs = prediction[\"framewise_output\"].detach(\n",
    "            ).cpu().numpy()[0]\n",
    "            \n",
    "    thresholded = framewise_outputs >= THRESHOLD\n",
    "\n",
    "    for target_idx in range(thresholded.shape[1]):\n",
    "        if thresholded[:, target_idx].mean() == 0:\n",
    "            pass\n",
    "        else:\n",
    "            detected = np.argwhere(thresholded[:, target_idx]).reshape(-1)\n",
    "            head_idx = 0\n",
    "            tail_idx = 0\n",
    "            while True:\n",
    "                if (tail_idx + 1 == len(detected)) or (\n",
    "                        detected[tail_idx + 1] - \n",
    "                        detected[tail_idx] != 1):\n",
    "                    onset = 0.01 * detected[\n",
    "                        head_idx] + global_time\n",
    "                    offset = 0.01 * detected[\n",
    "                        tail_idx] + global_time\n",
    "                    onset_idx = detected[head_idx]\n",
    "                    offset_idx = detected[tail_idx]\n",
    "                    max_confidence = framewise_outputs[\n",
    "                        onset_idx:offset_idx, target_idx].max()\n",
    "                    mean_confidence = framewise_outputs[\n",
    "                        onset_idx:offset_idx, target_idx].mean()\n",
    "                    estimated_event = {\n",
    "                        \"speech_recognition\": CODING[target_idx],\n",
    "                        \"start\": onset,\n",
    "                        \"end\": offset,\n",
    "                        \"max_confidence\": max_confidence,\n",
    "                        \"mean_confidence\": mean_confidence,\n",
    "                    }\n",
    "                    estimated_event_list.append(estimated_event)\n",
    "                    head_idx = tail_idx + 1\n",
    "                    tail_idx = tail_idx + 1\n",
    "                    if head_idx >= len(detected):\n",
    "                        break\n",
    "                else:\n",
    "                    tail_idx += 1\n",
    "    global_time += PERIOD\n",
    "    \n",
    "prediction_df = pd.DataFrame(estimated_event_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secure output file offset: max offset should be less than audio duration.\n",
    "\n",
    "max_offset = prediction_df.iloc[-1].end\n",
    "if max_offset > audio_duration:\n",
    "    prediction_df.iloc[-1].end = audio_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df[prediction_df.speech_recognition==\"speech\"].to_csv(OUTPUT_FILE, index=False)\n",
    "prediction_df.to_csv(OUTPUT_SOURCE_FILE, index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c1e26b787ea84c0136f54dc33c70b3c054a8e0cf94179623b14c46360f5a1e6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
