{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('asg': conda)",
      "metadata": {
        "interpreter": {
          "hash": "7c1e26b787ea84c0136f54dc33c70b3c054a8e0cf94179623b14c46360f5a1e6"
        }
      }
    },
    "colab": {
      "name": "train-cnn14decisionlevel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mans00rahmed/Automatic-Subtitle-File-Creation/blob/main/train_cnn14decisionlevel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# Train your model"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz0QdjNcQzNG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJaZtwuqQzNP"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "#from catalyst.dl import SupervisedRunner, State, CallbackOrder, Callback, CheckpointCallback"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def seed_all(s:int=42) -> None: \n",
        "    random.seed(s)\n",
        "    np.random.seed(s)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(s)\n",
        "    torch.manual_seed(s)\n",
        "    torch.cuda.manual_seed(s)  # type: ignore\n",
        "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
        "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
        "\n",
        "def seed_dataset(s:int=42) -> None:\n",
        "    random.seed(s)\n",
        "    \n",
        "\n",
        "class TrainingDirs(object):\n",
        "    \"\"\"\n",
        "    Initiate the working directory systems for training.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, dsname: str, pre_test: bool) -> None:\n",
        "        super().__init__()\n",
        "        ROOT = Path(os.path.split(os.path.abspath(\"\"))[0])\n",
        "        INPUT_ROOT = ROOT / 'data'\n",
        "        TARGET_AUDIO_DIR = INPUT_ROOT / dsname\n",
        "        print(f\"Working with dataset under {TARGET_AUDIO_DIR}.\")\n",
        "        assert os.path.exists(TARGET_AUDIO_DIR), \"Input dataset folder does not exist.\"\n",
        "        \n",
        "        self.dataset_folder = TARGET_AUDIO_DIR\n",
        "        self.train_folder = TARGET_AUDIO_DIR / 'train' if pre_test else TARGET_AUDIO_DIR\n",
        "        self.test_folder = TARGET_AUDIO_DIR / 'test' if pre_test else None"
      ],
      "metadata": {
        "id": "kJzFVI_gGT9h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "ROOT_PATH_ABS = os.path.split(os.path.abspath(\"\"))[0]"
      ],
      "metadata": {
        "id": "zvunWCoFG87l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.imports import *"
      ],
      "metadata": {
        "id": "havmuH2kOBjH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ModelConfig(object):\n",
        "    # Configurations for SED model.\n",
        "    sed_model_config = {\n",
        "        \"sample_rate\": 32000,\n",
        "        \"window_size\": 1024,\n",
        "        \"hop_size\": 320,\n",
        "        \"mel_bins\": 64,\n",
        "        \"fmin\": 50,\n",
        "        \"fmax\": 14000,\n",
        "        \"classes_num\": 527,\n",
        "    }\n",
        "    \n",
        "class DatasetConfig(object):\n",
        "    \"\"\"\n",
        "    Configuration for building your own dataset from sources.\n",
        "    \n",
        "    Attributes:\n",
        "        dataset_clip_time(int): Clip length for dataset. Default 2s.\n",
        "        dataset_sample_rate(int): Clip length for dataset. Default 32000.\n",
        "        dataset_audio_format(str): Clip format for dataset. Default using \"wav\"\n",
        "        sub_encoding(str): Use \"utf-8\" encoding for Aegisub subtitle support.\n",
        "    \"\"\"\n",
        "    \n",
        "    dataset_clip_time = 2 # seconds\n",
        "    \n",
        "    dataset_sample_rate = 32000\n",
        "    \n",
        "    dataset_audio_format = \"wav\" \n",
        "    \n",
        "    sub_encoding = \"utf-8\" # recommanded\n",
        "    "
      ],
      "metadata": {
        "id": "4U0YkaGsHQi-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2_btnhfHQzNQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "badd59f6-9d2a-4fa6-f734-4a52569d9f5c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-64ba454448e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcsrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainingDirs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mTD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcsrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigurations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcsrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigurations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mMC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcsrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseed_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'csrc'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from csrc.utils import TrainingDirs as TD\n",
        "from csrc.configurations import DatasetConfig as DC\n",
        "from csrc.configurations import ModelConfig as MC\n",
        "from csrc.utils import seed_dataset, seed_all "
      ]
    },
    {
      "source": [
        "## Train configurations"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "trmW4ZxAQzNR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "J8I52bXwQzNS"
      },
      "outputs": [],
      "source": [
        "# For better debugging.\n",
        "\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "y3QBf5gKQzNS",
        "outputId": "84f47115-6dbd-4329-cec3-f26dd50c36d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training clip length (sencods): 2\n"
          ]
        }
      ],
      "source": [
        "### The folder name of your dataset.\n",
        "DATASET = '/content/drive/My Drive/Auto-Subtitle-File-Generation/ASFG-train/data/standard-p2-32khz'\n",
        "\n",
        "### Whether you have split your dataset.\n",
        "### If False then the test dataset will be generated as configured in TrainParams and choose the split ratio.\n",
        "BUILD_TEST = False\n",
        "PREBUILD_TEST = False\n",
        "TEST_RATIO = 5\n",
        "\n",
        "### The ratio to split your train/validaion dataset.\n",
        "VALID_RATIO = 5\n",
        "### Whether to shuffle the dataset.\n",
        "SHUFFLE = True\n",
        "\n",
        "### Clip length that will be used for training.\n",
        "### Default to be the same as the audio clip length in the dataset.\n",
        "PERIOD = DatasetConfig.dataset_clip_time\n",
        "print(f\"Training clip length (sencods): {PERIOD}\")\n",
        "\n",
        "### Batch size for training. For example: 8gb GPU for 5s clips - batch size 32.\n",
        "BS = 48\n",
        "\n",
        "### Training epochs.\n",
        "EPOCHS = 30\n",
        "\n",
        "### Weights file path used for training.\n",
        "### Default under weights folder.\n",
        "WEIGHTS_PATH = \"./weights/Cnn14_DecisionLevelAtt_mAP0.425.pth\"\n",
        "\n",
        "### Default path to store your model.\n",
        "LOG_DIR = \"./train/logs/sp3-3/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "VbgeCZZaQzNU"
      },
      "outputs": [],
      "source": [
        "# Random seeding.\n",
        "# Change seed will change your validation set randomly picked from the dataset.\n",
        "\n",
        "SEED = 42\n",
        "seed_all(SEED)\n",
        "seed_dataset(SEED)"
      ]
    },
    {
      "source": [
        "## Process"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "97YWNoWmQzNV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LEUKEe1QQzNV",
        "outputId": "474d19d0-0155-4e87-d195-8b9cc0be4d32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working with dataset under /content/drive/My Drive/Auto-Subtitle-File-Generation/ASFG-train.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ee5e34196546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set up working folder for training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainingDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPREBUILD_TEST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mDATASET_FOLDER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mTRAIN_FOLDER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-d6f5be5f22fe>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dsname, pre_test)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mTARGET_AUDIO_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mINPUT_ROOT\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdsname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Working with dataset under {TARGET_AUDIO_DIR}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTARGET_AUDIO_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Input dataset folder does not exist.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTARGET_AUDIO_DIR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Input dataset folder does not exist."
          ]
        }
      ],
      "source": [
        "# Set up working folder for training.\n",
        "\n",
        "dirs = TrainingDirs(DATASET, PREBUILD_TEST)\n",
        "DATASET_FOLDER = dirs.dataset_folder\n",
        "TRAIN_FOLDER = dirs.train_folder\n",
        "TEST_FOLDER = dirs.test_folder\n",
        "\n",
        "### Currently we are training so we set up the training folder as the working folder.\n",
        "TRAIN_WORKING_FOLDER = TRAIN_FOLDER\n",
        "TEST_WORKING_FOLDER = TEST_FOLDER if TEST_FOLDER else TRAIN_FOLDER\n",
        "\n",
        "print(f\"FOLDER_FOR_TRAINING: {TRAIN_WORKING_FOLDER}\")\n",
        "print(f\"FOLDER_FOR_TEST: {TEST_WORKING_FOLDER}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHRec2aBQzNW",
        "outputId": "2d26150a-58a8-416a-ae09-873d9dec0620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files for training: 15228\nFiles for testing: 3806\n"
          ]
        }
      ],
      "source": [
        "# Train/Test split. If the test folder has not been manually selected, then split the test folder.\n",
        "\n",
        "def sort_index(x):\n",
        "    return int(x.split(\"-\")[0])\n",
        "\n",
        "if not TEST_FOLDER:\n",
        "    all_files = os.listdir(TRAIN_FOLDER)\n",
        "    all_files.sort(key=sort_index)\n",
        "    test_index = len(all_files) // TEST_RATIO\n",
        "    test_files = all_files[-test_index:]\n",
        "    train_files = all_files[:-test_index]\n",
        "else:\n",
        "    train_files = os.listdir(TRAIN_FOLDER)\n",
        "    test_files = os.listdir(TEST_FOLDER)\n",
        "\n",
        "print(f\"Files for training: {len(train_files)}\")\n",
        "print(f\"Files for testing: {len(test_files)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5IlkUBRQzNX",
        "outputId": "54de2a8c-7525-4a04-da37-7e1879c098f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files for training: 15228\nFiles for validation: 3806\nValidation file samples: ['2181-dallas-buyers-club-eng-1.wav', '2181-mission-impossible-iv-1.wav', '2181-the-dark-knight-rises-eng-0.wav', '2181-the-kingdom-of-heaven-eng-1.wav', '2181-the-kings-speech-eng-0.wav']\n"
          ]
        }
      ],
      "source": [
        "# Train/Validation split\n",
        "\n",
        "if SHUFFLE:\n",
        "    random.shuffle(train_files)\n",
        "\n",
        "if not BUILD_TEST:\n",
        "    train_files.extend(test_files)\n",
        "\n",
        "valid_idx = len(train_files) // VALID_RATIO\n",
        "valid_files = train_files[-valid_idx:]\n",
        "train_files = train_files[:-valid_idx]\n",
        "\n",
        "print(f\"Files for training: {len(train_files)}\")\n",
        "print(f\"Files for validation: {len(valid_files)}\")\n",
        "print(f\"Validation file samples: {valid_files[:5]}\")"
      ]
    },
    {
      "source": [
        "## Dataset"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "2jFaSOClQzNY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I982Qg9wQzNZ"
      },
      "outputs": [],
      "source": [
        "from csrc.dataset import PANNsDataset"
      ]
    },
    {
      "source": [
        "## Transformer"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "0PyJ5nrxQzNZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjI9N5qlQzNZ"
      },
      "outputs": [],
      "source": [
        "from csrc.transformers import BaseAug"
      ]
    },
    {
      "source": [
        "## Set up dataloader "
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "9F6pGN7OQzNa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJFkHLSCQzNa"
      },
      "outputs": [],
      "source": [
        "loaders = {\n",
        "    \"train\": data.DataLoader(PANNsDataset(train_files, training_folder=TRAIN_WORKING_FOLDER, test_folder=TEST_WORKING_FOLDER, waveform_transforms=BaseAug), # Build training set\n",
        "                            batch_size=BS,\n",
        "                            shuffle=True,\n",
        "                            num_workers=0, # 0 for windows system.\n",
        "                            pin_memory=True,\n",
        "                            drop_last=True),\n",
        "    \"valid\": data.DataLoader(PANNsDataset(valid_files, training_folder=TRAIN_WORKING_FOLDER, test_folder=TEST_WORKING_FOLDER, waveform_transforms=None), # Build training set.\\n\",\n",
        "                             batch_size=BS,\n",
        "                             shuffle=False,\n",
        "                             num_workers=0,\n",
        "                             pin_memory=True,\n",
        "                             drop_last=False)\n",
        "}"
      ]
    },
    {
      "source": [
        "## Model"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll61StYKQzNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\"\"\"\n",
        "Preprocessors for building models.\n",
        "\"\"\"\n",
        "\n",
        "class DFTBase(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"Base class for DFT and IDFT matrix\"\"\"\n",
        "        super(DFTBase, self).__init__()\n",
        "\n",
        "    def dft_matrix(self, n):\n",
        "        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n",
        "        omega = np.exp(-2 * np.pi * 1j / n)\n",
        "        W = np.power(omega, x * y)\n",
        "        return W\n",
        "\n",
        "    def idft_matrix(self, n):\n",
        "        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n",
        "        omega = np.exp(2 * np.pi * 1j / n)\n",
        "        W = np.power(omega, x * y)\n",
        "        return W\n",
        "    \n",
        "    \n",
        "class STFT(DFTBase):\n",
        "    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n",
        "        window=\"hann\", center=True, pad_mode=\"reflect\", freeze_parameters=True):\n",
        "        \"\"\"Implementation of STFT with Conv1d. The function has the same output \n",
        "        of librosa.core.stft\n",
        "        \"\"\"\n",
        "        super(STFT, self).__init__()\n",
        "\n",
        "        assert pad_mode in [\"constant\", \"reflect\"]\n",
        "\n",
        "        self.n_fft = n_fft\n",
        "        self.center = center\n",
        "        self.pad_mode = pad_mode\n",
        "\n",
        "        # By default, use the entire frame\n",
        "        if win_length is None:\n",
        "            win_length = n_fft\n",
        "\n",
        "        # Set the default hop, if it\"s not already specified\n",
        "        if hop_length is None:\n",
        "            hop_length = int(win_length // 4)\n",
        "\n",
        "        fft_window = librosa.filters.get_window(window, win_length, fftbins=True)\n",
        "\n",
        "        # Pad the window out to n_fft size\n",
        "        fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
        "\n",
        "        # DFT & IDFT matrix\n",
        "        self.W = self.dft_matrix(n_fft)\n",
        "\n",
        "        out_channels = n_fft // 2 + 1\n",
        "\n",
        "        self.conv_real = nn.Conv1d(in_channels=1, out_channels=out_channels, \n",
        "            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n",
        "            groups=1, bias=False)\n",
        "\n",
        "        self.conv_imag = nn.Conv1d(in_channels=1, out_channels=out_channels, \n",
        "            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n",
        "            groups=1, bias=False)\n",
        "\n",
        "        self.conv_real.weight.data = torch.Tensor(\n",
        "            np.real(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n",
        "        # (n_fft // 2 + 1, 1, n_fft)\n",
        "\n",
        "        self.conv_imag.weight.data = torch.Tensor(\n",
        "            np.imag(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n",
        "        # (n_fft // 2 + 1, 1, n_fft)\n",
        "\n",
        "        if freeze_parameters:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"input: (batch_size, data_length)\n",
        "        Returns:\n",
        "          real: (batch_size, n_fft // 2 + 1, time_steps)\n",
        "          imag: (batch_size, n_fft // 2 + 1, time_steps)\n",
        "        \"\"\"\n",
        "\n",
        "        x = input[:, None, :]   # (batch_size, channels_num, data_length)\n",
        "\n",
        "        if self.center:\n",
        "            x = F.pad(x, pad=(self.n_fft // 2, self.n_fft // 2), mode=self.pad_mode)\n",
        "\n",
        "        real = self.conv_real(x)\n",
        "        imag = self.conv_imag(x)\n",
        "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
        "\n",
        "        real = real[:, None, :, :].transpose(2, 3)\n",
        "        imag = imag[:, None, :, :].transpose(2, 3)\n",
        "        # (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
        "\n",
        "        return real, imag\n",
        "    \n",
        "    \n",
        "class Spectrogram(nn.Module):\n",
        "    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n",
        "        window=\"hann\", center=True, pad_mode=\"reflect\", power=2.0, \n",
        "        freeze_parameters=True):\n",
        "        \"\"\"Calculate spectrogram using pytorch. The STFT is implemented with \n",
        "        Conv1d. The function has the same output of librosa.core.stft\n",
        "        \"\"\"\n",
        "        super(Spectrogram, self).__init__()\n",
        "\n",
        "        self.power = power\n",
        "\n",
        "        self.stft = STFT(n_fft=n_fft, hop_length=hop_length, \n",
        "            win_length=win_length, window=window, center=center, \n",
        "            pad_mode=pad_mode, freeze_parameters=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"input: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
        "        Returns:\n",
        "          spectrogram: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
        "        \"\"\"\n",
        "\n",
        "        (real, imag) = self.stft.forward(input)\n",
        "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
        "\n",
        "        spectrogram = real ** 2 + imag ** 2\n",
        "\n",
        "        if self.power == 2.0:\n",
        "            pass\n",
        "        else:\n",
        "            spectrogram = spectrogram ** (self.power / 2.0)\n",
        "\n",
        "        return spectrogram\n",
        "\n",
        "    \n",
        "class LogmelFilterBank(nn.Module):\n",
        "    def __init__(self, sr=32000, n_fft=2048, n_mels=64, fmin=50, fmax=14000, is_log=True, \n",
        "        ref=1.0, amin=1e-10, top_db=80.0, freeze_parameters=True):\n",
        "        \"\"\"Calculate logmel spectrogram using pytorch. The mel filter bank is \n",
        "        the pytorch implementation of as librosa.filters.mel.\n",
        "        \n",
        "        This method is critical for different tasks. In this project,\n",
        "        we should set those params carefully to suit the task of human\n",
        "        voice recogniation.\n",
        "        1. n_mels: 128 and 64 are both reasonable. The performance has not yet been decided.\n",
        "        2. fmin and fmax: Should bigger than the range of human voice (100-10000hz).\n",
        "        3. sample rate: This must be set the same project-wise.\n",
        "        \"\"\"\n",
        "        super(LogmelFilterBank, self).__init__()\n",
        "\n",
        "        self.is_log = is_log\n",
        "        self.ref = ref\n",
        "        self.amin = amin\n",
        "        self.top_db = top_db\n",
        "\n",
        "        self.melW = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels,\n",
        "            fmin=fmin, fmax=fmax).T\n",
        "        # (n_fft // 2 + 1, mel_bins)\n",
        "\n",
        "        self.melW = nn.Parameter(torch.Tensor(self.melW))\n",
        "\n",
        "        if freeze_parameters:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"input: (batch_size, channels, time_steps)\n",
        "        \n",
        "        Output: (batch_size, time_steps, mel_bins)\n",
        "        \"\"\"\n",
        "\n",
        "        # Mel spectrogram\n",
        "        mel_spectrogram = torch.matmul(input, self.melW)\n",
        "\n",
        "        # Logmel spectrogram\n",
        "        if self.is_log:\n",
        "            output = self.power_to_db(mel_spectrogram)\n",
        "        else:\n",
        "            output = mel_spectrogram\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def power_to_db(self, input):\n",
        "        \"\"\"Power to db, this function is the pytorch implementation of \n",
        "        librosa.core.power_to_lb\n",
        "        \"\"\"\n",
        "        ref_value = self.ref\n",
        "        log_spec = 10.0 * torch.log10(torch.clamp(input, min=self.amin, max=np.inf))\n",
        "        log_spec -= 10.0 * np.log10(np.maximum(self.amin, ref_value))\n",
        "\n",
        "        if self.top_db is not None:\n",
        "            if self.top_db < 0:\n",
        "                raise ValueError(\"top_db must be non-negative\")\n",
        "            log_spec = torch.clamp(log_spec, min=log_spec.max().item() - self.top_db, max=np.inf)\n",
        "\n",
        "        return log_spec\n",
        "    \n",
        "class DropStripes(nn.Module):\n",
        "    def __init__(self, dim, drop_width, stripes_num):\n",
        "        \"\"\"Drop stripes. \n",
        "        Args:\n",
        "          dim: int, dimension along which to drop\n",
        "          drop_width: int, maximum width of stripes to drop\n",
        "          stripes_num: int, how many stripes to drop\n",
        "        \"\"\"\n",
        "        super(DropStripes, self).__init__()\n",
        "\n",
        "        assert dim in [2, 3]    # dim 2: time; dim 3: frequency\n",
        "\n",
        "        self.dim = dim\n",
        "        self.drop_width = drop_width\n",
        "        self.stripes_num = stripes_num\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"input: (batch_size, channels, time_steps, freq_bins)\"\"\"\n",
        "\n",
        "        assert input.ndimension() == 4\n",
        "\n",
        "        if self.training is False:\n",
        "            return input\n",
        "\n",
        "        else:\n",
        "            batch_size = input.shape[0]\n",
        "            total_width = input.shape[self.dim]\n",
        "\n",
        "            for n in range(batch_size):\n",
        "                self.transform_slice(input[n], total_width)\n",
        "\n",
        "            return input\n",
        "\n",
        "\n",
        "    def transform_slice(self, e, total_width):\n",
        "        \"\"\"e: (channels, time_steps, freq_bins)\"\"\"\n",
        "\n",
        "        for _ in range(self.stripes_num):\n",
        "            distance = torch.randint(low=0, high=self.drop_width, size=(1,))[0]\n",
        "            bgn = torch.randint(low=0, high=total_width - distance, size=(1,))[0]\n",
        "\n",
        "            if self.dim == 2:\n",
        "                e[:, bgn : bgn + distance, :] = 0\n",
        "            elif self.dim == 3:\n",
        "                e[:, :, bgn : bgn + distance] = 0\n",
        "\n",
        "\n",
        "class SpecAugmentation(nn.Module):\n",
        "    def __init__(self, time_drop_width, time_stripes_num, freq_drop_width, \n",
        "        freq_stripes_num):\n",
        "        \"\"\"Spec augmetation. \n",
        "        [ref] Park, D.S., Chan, W., Zhang, Y., Chiu, C.C., Zoph, B., Cubuk, E.D. \n",
        "        and Le, Q.V., 2019. Specaugment: A simple data augmentation method \n",
        "        for automatic speech recognition. arXiv preprint arXiv:1904.08779.\n",
        "        Args:\n",
        "          time_drop_width: int\n",
        "          time_stripes_num: int\n",
        "          freq_drop_width: int\n",
        "          freq_stripes_num: int\n",
        "        \"\"\"\n",
        "\n",
        "        super(SpecAugmentation, self).__init__()\n",
        "\n",
        "        self.time_dropper = DropStripes(dim=2, drop_width=time_drop_width, \n",
        "            stripes_num=time_stripes_num)\n",
        "\n",
        "        self.freq_dropper = DropStripes(dim=3, drop_width=freq_drop_width, \n",
        "            stripes_num=freq_stripes_num)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.time_dropper(input)\n",
        "        x = self.freq_dropper(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Building models.\n",
        "\"\"\"\n",
        "\n",
        "def init_layer(layer):\n",
        "    nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "    if hasattr(layer, \"bias\"):\n",
        "        if layer.bias is not None:\n",
        "            layer.bias.data.fill_(0.)\n",
        "\n",
        "\n",
        "def init_bn(bn):\n",
        "    bn.bias.data.fill_(0.)\n",
        "    bn.weight.data.fill_(1.0)\n",
        "\n",
        "\n",
        "def init_weights(model):\n",
        "    classname = model.__class__.__name__\n",
        "    if classname.find(\"Conv2d\") != -1:\n",
        "        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n",
        "        model.bias.data.fill_(0)\n",
        "    elif classname.find(\"BatchNorm\") != -1:\n",
        "        model.weight.data.normal_(1.0, 0.02)\n",
        "        model.bias.data.fill_(0)\n",
        "    elif classname.find(\"GRU\") != -1:\n",
        "        for weight in model.parameters():\n",
        "            if len(weight.size()) > 1:\n",
        "                nn.init.orghogonal_(weight.data)\n",
        "    elif classname.find(\"Linear\") != -1:\n",
        "        model.weight.data.normal_(0, 0.01)\n",
        "        model.bias.data.zero_()\n",
        "\n",
        "\n",
        "def do_mixup(x: torch.Tensor, mixup_lambda: torch.Tensor):\n",
        "    \"\"\"Mixup x of even indexes (0, 2, 4, ...) with x of odd indexes\n",
        "    (1, 3, 5, ...).\n",
        "    Args:\n",
        "      x: (batch_size * 2, ...)\n",
        "      mixup_lambda: (batch_size * 2,)\n",
        "    Returns:\n",
        "      out: (batch_size, ...)\n",
        "    \"\"\"\n",
        "    out = (x[0::2].transpose(0, -1) * mixup_lambda[0::2] +\n",
        "           x[1::2].transpose(0, -1) * mixup_lambda[1::2]).transpose(0, -1)\n",
        "    return out\n",
        "\n",
        "\n",
        "class Mixup(object):\n",
        "    def __init__(self, mixup_alpha, random_seed=1234):\n",
        "        \"\"\"Mixup coefficient generator.\n",
        "        \"\"\"\n",
        "        self.mixup_alpha = mixup_alpha\n",
        "        self.random_state = np.random.RandomState(random_seed)\n",
        "\n",
        "    def get_lambda(self, batch_size):\n",
        "        \"\"\"Get mixup random coefficients.\n",
        "        Args:\n",
        "          batch_size: int\n",
        "        Returns:\n",
        "          mixup_lambdas: (batch_size,)\n",
        "        \"\"\"\n",
        "        mixup_lambdas = []\n",
        "        for n in range(0, batch_size, 2):\n",
        "            lam = self.random_state.beta(self.mixup_alpha, self.mixup_alpha, 1)[0]\n",
        "            mixup_lambdas.append(lam)\n",
        "            mixup_lambdas.append(1. - lam)\n",
        "\n",
        "        return torch.from_numpy(np.array(mixup_lambdas, dtype=np.float32))\n",
        "\n",
        "\n",
        "def interpolate(x: torch.Tensor, ratio: int):\n",
        "    \"\"\"Interpolate data in time domain. This is used to compensate the\n",
        "    resolution reduction in downsampling of a CNN.\n",
        "    Args:\n",
        "      x: (batch_size, time_steps, classes_num)\n",
        "      ratio: int, ratio to interpolate\n",
        "    Returns:\n",
        "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
        "    \"\"\"\n",
        "    (batch_size, time_steps, classes_num) = x.shape\n",
        "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
        "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
        "    return upsampled\n",
        "\n",
        "\n",
        "def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n",
        "    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n",
        "    is the same as the value of the last frame.\n",
        "    Args:\n",
        "      framewise_output: (batch_size, frames_num, classes_num)\n",
        "      frames_num: int, number of frames to pad\n",
        "    Outputs:\n",
        "      output: (batch_size, frames_num, classes_num)\n",
        "    \"\"\"\n",
        "    pad = framewise_output[:, -1:, :].repeat(\n",
        "        1, frames_num - framewise_output.shape[1], 1)\n",
        "    \"\"\"tensor for padding\"\"\"\n",
        "\n",
        "    output = torch.cat((framewise_output, pad), dim=1)\n",
        "    \"\"\"(batch_size, frames_num, classes_num)\"\"\"\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1),\n",
        "            bias=False)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=out_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1),\n",
        "            bias=False)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        init_layer(self.conv1)\n",
        "        init_layer(self.conv2)\n",
        "        init_bn(self.bn1)\n",
        "        init_bn(self.bn2)\n",
        "\n",
        "    def forward(self, input, pool_size=(2, 2), pool_type=\"avg\"):\n",
        "\n",
        "        x = input\n",
        "        x = F.relu_(self.bn1(self.conv1(x)))\n",
        "        x = F.relu_(self.bn2(self.conv2(x)))\n",
        "        if pool_type == \"max\":\n",
        "            x = F.max_pool2d(x, kernel_size=pool_size)\n",
        "        elif pool_type == \"avg\":\n",
        "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
        "        elif pool_type == \"avg+max\":\n",
        "            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n",
        "            x2 = F.max_pool2d(x, kernel_size=pool_size)\n",
        "            x = x1 + x2\n",
        "        else:\n",
        "            raise Exception(\"Incorrect argument!\")\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features: int,\n",
        "                 out_features: int,\n",
        "                 activation=\"linear\",\n",
        "                 temperature=1.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.activation = activation\n",
        "        self.temperature = temperature\n",
        "        self.att = nn.Conv1d(\n",
        "            in_channels=in_features,\n",
        "            out_channels=out_features,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=True)\n",
        "        self.cla = nn.Conv1d(\n",
        "            in_channels=in_features,\n",
        "            out_channels=out_features,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=True)\n",
        "\n",
        "        self.bn_att = nn.BatchNorm1d(out_features)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        init_layer(self.att)\n",
        "        init_layer(self.cla)\n",
        "        init_bn(self.bn_att)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (n_samples, n_in, n_time)\n",
        "        norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n",
        "        cla = self.nonlinear_transform(self.cla(x))\n",
        "        x = torch.sum(norm_att * cla, dim=2)\n",
        "        return x, norm_att, cla\n",
        "\n",
        "    def nonlinear_transform(self, x):\n",
        "        if self.activation == \"linear\":\n",
        "            return x\n",
        "        elif self.activation == \"sigmoid\":\n",
        "            return torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class AttBlockV2(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features: int,\n",
        "                 out_features: int,\n",
        "                 activation=\"linear\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.activation = activation\n",
        "        self.att = nn.Conv1d(\n",
        "            in_channels=in_features,\n",
        "            out_channels=out_features,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=True)\n",
        "        self.cla = nn.Conv1d(\n",
        "            in_channels=in_features,\n",
        "            out_channels=out_features,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=True)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        init_layer(self.att)\n",
        "        init_layer(self.cla)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (n_samples, n_in, n_time)\n",
        "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
        "        cla = self.nonlinear_transform(self.cla(x))\n",
        "        x = torch.sum(norm_att * cla, dim=2)\n",
        "        return x, norm_att, cla\n",
        "\n",
        "    def nonlinear_transform(self, x):\n",
        "        if self.activation == \"linear\":\n",
        "            return x\n",
        "        elif self.activation == \"sigmoid\":\n",
        "            return torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class PANNsCNN14Att(nn.Module):\n",
        "    def __init__(self, sample_rate: int, window_size: int, hop_size: int,\n",
        "                 mel_bins: int, fmin: int, fmax: int, classes_num: int):\n",
        "        super().__init__()\n",
        "\n",
        "        window = \"hann\"\n",
        "        center = True\n",
        "        pad_mode = \"reflect\"\n",
        "        ref = 1.0\n",
        "        amin = 1e-10\n",
        "        top_db = None\n",
        "        self.interpolate_ratio = 32  # Downsampled ratio\n",
        "\n",
        "        # Spectrogram extractor\n",
        "        self.spectrogram_extractor = Spectrogram(\n",
        "            n_fft=window_size,\n",
        "            hop_length=hop_size,\n",
        "            win_length=window_size,\n",
        "            window=window,\n",
        "            center=center,\n",
        "            pad_mode=pad_mode,\n",
        "            freeze_parameters=True)\n",
        "\n",
        "        # Logmel feature extractor\n",
        "        self.logmel_extractor = LogmelFilterBank(\n",
        "            sr=sample_rate,\n",
        "            n_fft=window_size,\n",
        "            n_mels=mel_bins,\n",
        "            fmin=fmin,\n",
        "            fmax=fmax,\n",
        "            ref=ref,\n",
        "            amin=amin,\n",
        "            top_db=top_db,\n",
        "            freeze_parameters=True)\n",
        "\n",
        "        # Spec augmenter\n",
        "        self.spec_augmenter = SpecAugmentation(\n",
        "            time_drop_width=64,\n",
        "            time_stripes_num=2,\n",
        "            freq_drop_width=8,\n",
        "            freq_stripes_num=2)\n",
        "\n",
        "        self.bn0 = nn.BatchNorm2d(mel_bins)\n",
        "\n",
        "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
        "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
        "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
        "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
        "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
        "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
        "\n",
        "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
        "        self.att_block = AttBlock(2048, classes_num, activation=\"sigmoid\")\n",
        "\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        init_bn(self.bn0)\n",
        "        init_layer(self.fc1)\n",
        "\n",
        "    def forward(self, input, mixup_lambda=None):\n",
        "        \"\"\"\n",
        "        Input: (batch_size, data_length)\"\"\"\n",
        "\n",
        "        # t1 = time.time()\n",
        "        x = self.spectrogram_extractor(\n",
        "            input)  # (batch_size, 1, time_steps, freq_bins)\n",
        "        x = self.logmel_extractor(x)  # (batch_size, 1, time_steps, mel_bins)\n",
        "\n",
        "        frames_num = x.shape[2]\n",
        "\n",
        "        x = x.transpose(1, 3)\n",
        "        x = self.bn0(x)\n",
        "        x = x.transpose(1, 3)\n",
        "\n",
        "        if self.training:\n",
        "            x = self.spec_augmenter(x)\n",
        "\n",
        "        # Mixup on spectrogram\n",
        "        if self.training and mixup_lambda is not None:\n",
        "            x = do_mixup(x, mixup_lambda)\n",
        "\n",
        "        x = self.conv_block1(x, pool_size=(2, 2), pool_type=\"avg\")\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv_block2(x, pool_size=(2, 2), pool_type=\"avg\")\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv_block3(x, pool_size=(2, 2), pool_type=\"avg\")\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv_block4(x, pool_size=(2, 2), pool_type=\"avg\")\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv_block5(x, pool_size=(2, 2), pool_type=\"avg\")\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv_block6(x, pool_size=(1, 1), pool_type=\"avg\")\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = torch.mean(x, dim=3)\n",
        "\n",
        "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
        "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
        "        x = x1 + x2\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = F.relu_(self.fc1(x))\n",
        "        x = x.transpose(1, 2)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
        "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
        "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
        "\n",
        "        # Get framewise output\n",
        "        framewise_output = interpolate(segmentwise_output,\n",
        "                                       self.interpolate_ratio)\n",
        "        framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
        "\n",
        "        output_dict = {\n",
        "            \"framewise_output\": framewise_output,\n",
        "            \"logit\": logit,\n",
        "            \"clipwise_output\": clipwise_output\n",
        "        }\n",
        "\n",
        "        return output_dict\n",
        "    "
      ],
      "metadata": {
        "id": "T-4ZN3NVQG1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifITAuIfQzNb"
      },
      "outputs": [],
      "source": [
        "from csrc.models import AttBlock, PANNsCNN14Att"
      ]
    },
    {
      "source": [
        "## Loss"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "6upY_4IQQzNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class PANNsLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bce = nn.BCELoss()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input_ = input[\"clipwise_output\"]\n",
        "        input_ = torch.where(torch.isnan(input_),\n",
        "                             torch.zeros_like(input_),\n",
        "                             input_)\n",
        "        input_ = torch.where(torch.isinf(input_),\n",
        "                             torch.zeros_like(input_),\n",
        "                             input_)\n",
        "\n",
        "        target = target.float()\n",
        "\n",
        "        input_ = torch.clamp(input_, 0.0, 1.0)\n",
        "        \n",
        "        return self.bce(input_, target)\n",
        "    \n",
        "    \n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, logit, target):\n",
        "        target = target.float()\n",
        "        max_val = (-logit).clamp(min=0)\n",
        "        loss = logit - logit * target + max_val + \\\n",
        "            ((-max_val).exp() + (-logit - max_val).exp()).log()\n",
        "\n",
        "        invprobs = F.logsigmoid(-logit * (target * 2.0 - 1.0))\n",
        "        loss = (invprobs * self.gamma).exp() * loss\n",
        "        if len(loss.size()) == 2:\n",
        "            loss = loss.sum(dim=1)\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "class ImprovedFocalLoss(nn.Module):\n",
        "    def __init__(self, weights=[1, 1]):\n",
        "        super().__init__()\n",
        "\n",
        "        self.focal = FocalLoss()\n",
        "        self.weights = weights\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input_ = input[\"logit\"]\n",
        "        target = target.float()\n",
        "\n",
        "        framewise_output = input[\"framewise_logit\"]\n",
        "        clipwise_output_with_max, _ = framewise_output.max(dim=1)\n",
        "\n",
        "        normal_loss = self.focal(input_, target)\n",
        "        auxiliary_loss = self.focal(clipwise_output_with_max, target)\n",
        "\n",
        "        return self.weights[0] * normal_loss + self.weights[1] * auxiliary_loss\n",
        "\n",
        "\n",
        "class ImprovedPANNsLoss(nn.Module):\n",
        "    def __init__(self, output_key=\"logit\", weights=[1, 1]):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_key = output_key\n",
        "        if output_key == \"logit\":\n",
        "            self.normal_loss = nn.BCEWithLogitsLoss()\n",
        "        else:\n",
        "            self.normal_loss = nn.BCELoss()\n",
        "\n",
        "        self.weights = weights\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input_ = input[self.output_key]\n",
        "        target = target.float()\n",
        "\n",
        "        framewise_output = input[\"framewise_output\"]\n",
        "        clipwise_output_with_max, _ = framewise_output.max(dim=1)\n",
        "\n",
        "        normal_loss = self.normal_loss(input_, target)\n",
        "        # auxiliary_loss = self.bce(clipwise_output_with_max, target)\n",
        "        auxiliary_loss = self.normal_loss(clipwise_output_with_max, target)\n",
        "\n",
        "        return self.weights[0] * normal_loss + self.weights[1] * auxiliary_loss"
      ],
      "metadata": {
        "id": "VWSbPayvQOX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzU8PZvBQzNc"
      },
      "outputs": [],
      "source": [
        "from csrc.losses import ImprovedPANNsLoss"
      ]
    },
    {
      "source": [
        "## Callbacks"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "aGVhpt7ZQzNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, average_precision_score, precision_score\n",
        "\n",
        "\n",
        "class F1Callback(Callback):\n",
        "    def __init__(self,\n",
        "                 input_key: str = \"targets\",\n",
        "                 output_key: str = \"logits\",\n",
        "                 model_output_key: str = \"clipwise_output\",\n",
        "                 prefix: str = \"macro_f1\"):\n",
        "        super().__init__(CallbackOrder.Metric)\n",
        "\n",
        "        self.input_key = input_key\n",
        "        self.output_key = output_key\n",
        "        self.model_output_key = model_output_key\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def on_loader_start(self, state: IRunner):\n",
        "        self.prediction: List[np.ndarray] = []\n",
        "        self.target: List[np.ndarray] = []\n",
        "\n",
        "    def on_batch_end(self, state: IRunner):\n",
        "        targ = state.batch[self.input_key].detach().cpu().numpy()\n",
        "        out = state.batch[self.output_key]\n",
        "\n",
        "        clipwise_output = out[self.model_output_key].detach().cpu().numpy()\n",
        "\n",
        "        self.prediction.append(clipwise_output)\n",
        "        self.target.append(targ)\n",
        "\n",
        "        y_pred = clipwise_output.argmax(axis=1)\n",
        "        y_true = targ.argmax(axis=1)\n",
        "\n",
        "        score = f1_score(y_true, y_pred, average=\"macro\")\n",
        "        state.batch_metrics[self.prefix] = score\n",
        "\n",
        "    def on_loader_end(self, state: IRunner):\n",
        "        y_pred = np.concatenate(self.prediction, axis=0).argmax(axis=1)\n",
        "        y_true = np.concatenate(self.target, axis=0).argmax(axis=1)\n",
        "        score = f1_score(y_true, y_pred, average=\"macro\")\n",
        "        state.loader_metrics[self.prefix] = score\n",
        "        if state.is_valid_loader:\n",
        "            state.epoch_metrics[\"valid\"] = {\n",
        "                self.prefix: score\n",
        "            }\n",
        "        else:\n",
        "            state.epoch_metrics[\"train\"] = {\n",
        "                self.prefix: score\n",
        "            }            \n",
        "            \n",
        "class PrecisionCallback(Callback):\n",
        "    def __init__(self,\n",
        "                 input_key: str = \"targets\",\n",
        "                 output_key: str = \"logits\",\n",
        "                 model_output_key: str = \"clipwise_output\",\n",
        "                 prefix: str = \"precision\"):\n",
        "        super().__init__(CallbackOrder.Metric)\n",
        "\n",
        "        self.input_key = input_key\n",
        "        self.output_key = output_key\n",
        "        self.model_output_key = model_output_key\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def on_loader_start(self, state: IRunner):\n",
        "        self.prediction: List[np.ndarray] = []\n",
        "        self.target: List[np.ndarray] = []\n",
        "\n",
        "    def on_batch_end(self, state: IRunner):\n",
        "        targ = state.batch[self.input_key].detach().cpu().numpy()\n",
        "        out = state.batch[self.output_key]\n",
        "\n",
        "        clipwise_output = out[self.model_output_key].detach().cpu().numpy()\n",
        "\n",
        "        self.prediction.append(clipwise_output)\n",
        "        self.target.append(targ)\n",
        "\n",
        "        y_pred = clipwise_output.argmax(axis=1)\n",
        "        y_true = targ.argmax(axis=1)\n",
        "\n",
        "        score = precision_score(y_true, y_pred)\n",
        "        state.batch_metrics[self.prefix] = score\n",
        "\n",
        "    def on_loader_end(self, state: IRunner):\n",
        "        y_pred = np.concatenate(self.prediction, axis=0).argmax(axis=1)\n",
        "        y_true = np.concatenate(self.target, axis=0).argmax(axis=1)\n",
        "        score = precision_score(y_true, y_pred)\n",
        "        state.loader_metrics[self.prefix] = score\n",
        "        if state.is_valid_loader:\n",
        "            state.epoch_metrics[\"valid\"] = {\n",
        "                self.prefix: score\n",
        "            }\n",
        "        else:\n",
        "            state.epoch_metrics[\"train\"] = {\n",
        "                self.prefix: score\n",
        "            }  \n",
        "            \n",
        "            \n",
        "class mAPCallback(Callback):\n",
        "    def __init__(self,\n",
        "                 input_key: str = \"targets\",\n",
        "                 output_key: str = \"logits\",\n",
        "                 model_output_key: str = \"clipwise_output\",\n",
        "                 prefix: str = \"mAP\"):\n",
        "        super().__init__(CallbackOrder.Metric)\n",
        "        self.input_key = input_key\n",
        "        self.output_key = output_key\n",
        "        self.model_output_key = model_output_key\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def on_loader_start(self, state: IRunner):\n",
        "        self.prediction: List[np.ndarray] = []\n",
        "        self.target: List[np.ndarray] = []\n",
        "\n",
        "    def on_batch_end(self, state: IRunner):\n",
        "        targ = state.batch[self.input_key].detach().cpu().numpy()\n",
        "        out = state.batch[self.output_key]\n",
        "\n",
        "        clipwise_output = out[self.model_output_key].detach().cpu().numpy()\n",
        "\n",
        "        self.prediction.append(clipwise_output)\n",
        "        self.target.append(targ)\n",
        "\n",
        "        targ = np.nan_to_num(targ)\n",
        "        clipwise_output = np.nan_to_num(clipwise_output)\n",
        "        score = average_precision_score(targ, clipwise_output, average=None)\n",
        "        score = np.nan_to_num(score).mean()\n",
        "        state.batch_metrics[self.prefix] = score\n",
        "\n",
        "    def on_loader_end(self, state: IRunner):\n",
        "        y_pred = np.concatenate(self.prediction, axis=0)\n",
        "        y_true = np.concatenate(self.target, axis=0)\n",
        "        score = average_precision_score(y_true, y_pred, average=None)\n",
        "        score = np.nan_to_num(score).mean()\n",
        "        state.loader_metrics[self.prefix] = score\n",
        "        if state.is_valid_loader:\n",
        "            state.epoch_metrics[\"valid\"] = {\n",
        "                self.prefix: score\n",
        "            }\n",
        "        else:\n",
        "            state.epoch_metrics[\"train\"] = {\n",
        "                self.prefix: score\n",
        "            }    \n"
      ],
      "metadata": {
        "id": "ZRyPTiEsQYlf",
        "outputId": "ca525ddb-8aee-406d-db81-d0ab41d28ec0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-fb83c788c0a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mF1Callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     def __init__(self,\n\u001b[1;32m      6\u001b[0m                  \u001b[0minput_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Callback' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdcndVJjQzNc"
      },
      "outputs": [],
      "source": [
        "from csrc.callbacks import F1Callback, mAPCallback, PrecisionCallback"
      ]
    },
    {
      "source": [
        "## Training Configurations"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "3li9PWrxQzNd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBYgEBmVQzNd"
      },
      "outputs": [],
      "source": [
        "# device\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# model\n",
        "model = PANNsCNN14Att(**MC.sed_model_config)\n",
        "weights = torch.load(WEIGHTS_PATH)\n",
        "model.load_state_dict(weights[\"model\"])\n",
        "model.att_block = AttBlock(2048, 2, activation=\"sigmoid\")\n",
        "model.att_block.init_weights()\n",
        "model.to(device)\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# scheduler\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "# loss\n",
        "loss = ImprovedPANNsLoss().to(device)\n",
        "\n",
        "# callbacks\n",
        "callbacks = [\n",
        "    F1Callback(),\n",
        "    mAPCallback(),\n",
        "    PrecisionCallback(),\n",
        "    CheckpointCallback(save_n_best=3),\n",
        "]"
      ]
    },
    {
      "source": [
        "## Training"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "EQpny9gMQzNd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "EuHujPcSQzNd",
        "outputId": "e8332d94-c35b-4782-ce16-449e2c0142ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/30 * Epoch (train): 100% 317/317 [12:17<00:00,  2.33s/it, loss=1.165, mAP=0.835, macro_f1=0.723, precision=0.913]\n",
            "1/30 * Epoch (valid): 100% 80/80 [00:20<00:00,  3.94it/s, loss=1.796, mAP=0.500, macro_f1=0.364, precision=0.000e+00]\n",
            "[2021-02-12 10:42:02,018] \n",
            "1/30 * Epoch 1 (_base): lr=0.0010 | momentum=0.9000\n",
            "1/30 * Epoch 1 (train): epoch_mAP=0.8601 | epoch_macro_f1=0.8238 | epoch_precision=0.8784 | loss=1.1902 | mAP=0.8735 | macro_f1=0.8127 | precision=0.8863\n",
            "1/30 * Epoch 1 (valid): epoch_mAP=0.9348 | epoch_macro_f1=0.8901 | epoch_precision=0.8869 | loss=0.8952 | mAP=0.8876 | macro_f1=0.8527 | precision=0.8097\n",
            "2/30 * Epoch (train): 100% 317/317 [11:50<00:00,  2.24s/it, loss=1.044, mAP=0.918, macro_f1=0.838, precision=0.875]\n",
            "2/30 * Epoch (valid): 100% 80/80 [00:16<00:00,  4.77it/s, loss=1.368, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 10:54:13,367] \n",
            "2/30 * Epoch 2 (_base): lr=0.0009 | momentum=0.9000\n",
            "2/30 * Epoch 2 (train): epoch_mAP=0.8825 | epoch_macro_f1=0.8561 | epoch_precision=0.9126 | loss=0.9780 | mAP=0.8994 | macro_f1=0.8540 | precision=0.9128\n",
            "2/30 * Epoch 2 (valid): epoch_mAP=0.9332 | epoch_macro_f1=0.8901 | epoch_precision=0.8646 | loss=0.8896 | mAP=0.8878 | macro_f1=0.8479 | precision=0.7912\n",
            "3/30 * Epoch (train): 100% 317/317 [11:03<00:00,  2.09s/it, loss=0.717, mAP=0.997, macro_f1=0.978, precision=1.000]\n",
            "3/30 * Epoch (valid): 100% 80/80 [00:15<00:00,  5.01it/s, loss=1.741, mAP=0.500, macro_f1=0.391, precision=0.000e+00]\n",
            "[2021-02-12 11:05:35,957] \n",
            "3/30 * Epoch 3 (_base): lr=0.0008 | momentum=0.9000\n",
            "3/30 * Epoch 3 (train): epoch_mAP=0.8975 | epoch_macro_f1=0.8629 | epoch_precision=0.9127 | loss=0.9476 | mAP=0.9088 | macro_f1=0.8602 | precision=0.9132\n",
            "3/30 * Epoch 3 (valid): epoch_mAP=0.9342 | epoch_macro_f1=0.8927 | epoch_precision=0.8683 | loss=0.8590 | mAP=0.8909 | macro_f1=0.8557 | precision=0.7875\n",
            "4/30 * Epoch (train): 100% 317/317 [10:53<00:00,  2.06s/it, loss=0.921, mAP=0.867, macro_f1=0.849, precision=0.969]\n",
            "4/30 * Epoch (valid): 100% 80/80 [00:15<00:00,  5.03it/s, loss=1.341, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 11:16:48,982] \n",
            "4/30 * Epoch 4 (_base): lr=0.0007 | momentum=0.9000\n",
            "4/30 * Epoch 4 (train): epoch_mAP=0.9052 | epoch_macro_f1=0.8678 | epoch_precision=0.9168 | loss=0.9282 | mAP=0.9171 | macro_f1=0.8657 | precision=0.9160\n",
            "4/30 * Epoch 4 (valid): epoch_mAP=0.9314 | epoch_macro_f1=0.8969 | epoch_precision=0.8914 | loss=0.8787 | mAP=0.8856 | macro_f1=0.8667 | precision=0.8069\n",
            "5/30 * Epoch (train): 100% 317/317 [10:53<00:00,  2.06s/it, loss=0.750, mAP=0.995, macro_f1=0.916, precision=1.000]\n",
            "5/30 * Epoch (valid): 100% 80/80 [00:15<00:00,  5.02it/s, loss=1.356, mAP=0.500, macro_f1=0.391, precision=0.000e+00]\n",
            "[2021-02-12 11:28:02,690] \n",
            "5/30 * Epoch 5 (_base): lr=0.0005 | momentum=0.9000\n",
            "5/30 * Epoch 5 (train): epoch_mAP=0.9125 | epoch_macro_f1=0.8733 | epoch_precision=0.9222 | loss=0.9106 | mAP=0.9215 | macro_f1=0.8705 | precision=0.9216\n",
            "5/30 * Epoch 5 (valid): epoch_mAP=0.9312 | epoch_macro_f1=0.8888 | epoch_precision=0.8735 | loss=0.8612 | mAP=0.8858 | macro_f1=0.8448 | precision=0.7927\n",
            "6/30 * Epoch (train): 100% 317/317 [10:57<00:00,  2.07s/it, loss=0.816, mAP=0.964, macro_f1=0.876, precision=0.939]\n",
            "6/30 * Epoch (valid): 100% 80/80 [00:16<00:00,  5.00it/s, loss=1.594, mAP=0.500, macro_f1=0.391, precision=0.000e+00]\n",
            "[2021-02-12 11:39:19,569] \n",
            "6/30 * Epoch 6 (_base): lr=0.0003 | momentum=0.9000\n",
            "6/30 * Epoch 6 (train): epoch_mAP=0.9168 | epoch_macro_f1=0.8790 | epoch_precision=0.9257 | loss=0.8943 | mAP=0.9249 | macro_f1=0.8767 | precision=0.9258\n",
            "6/30 * Epoch 6 (valid): epoch_mAP=0.9381 | epoch_macro_f1=0.8920 | epoch_precision=0.8602 | loss=0.8531 | mAP=0.8919 | macro_f1=0.8465 | precision=0.7739\n",
            "7/30 * Epoch (train): 100% 317/317 [10:53<00:00,  2.06s/it, loss=0.812, mAP=0.976, macro_f1=0.917, precision=0.957]\n",
            "7/30 * Epoch (valid): 100% 80/80 [00:16<00:00,  5.00it/s, loss=1.440, mAP=0.500, macro_f1=0.391, precision=0.000e+00]\n",
            "[2021-02-12 11:50:32,105] \n",
            "7/30 * Epoch 7 (_base): lr=0.0002 | momentum=0.9000\n",
            "7/30 * Epoch 7 (train): epoch_mAP=0.9231 | epoch_macro_f1=0.8824 | epoch_precision=0.9264 | loss=0.8797 | mAP=0.9299 | macro_f1=0.8806 | precision=0.9262\n",
            "7/30 * Epoch 7 (valid): epoch_mAP=0.9328 | epoch_macro_f1=0.8942 | epoch_precision=0.8735 | loss=0.8578 | mAP=0.8851 | macro_f1=0.8506 | precision=0.7968\n",
            "8/30 * Epoch (train): 100% 317/317 [10:59<00:00,  2.08s/it, loss=0.784, mAP=0.949, macro_f1=0.885, precision=0.935]\n",
            "8/30 * Epoch (valid): 100% 80/80 [00:15<00:00,  5.02it/s, loss=1.258, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 12:01:50,759] \n",
            "8/30 * Epoch 8 (_base): lr=9.549e-05 | momentum=0.9000\n",
            "8/30 * Epoch 8 (train): epoch_mAP=0.9301 | epoch_macro_f1=0.8865 | epoch_precision=0.9306 | loss=0.8657 | mAP=0.9358 | macro_f1=0.8842 | precision=0.9304\n",
            "8/30 * Epoch 8 (valid): epoch_mAP=0.9383 | epoch_macro_f1=0.9023 | epoch_precision=0.8854 | loss=0.8393 | mAP=0.8908 | macro_f1=0.8653 | precision=0.7985\n",
            "9/30 * Epoch (train): 100% 317/317 [11:04<00:00,  2.10s/it, loss=0.913, mAP=0.921, macro_f1=0.863, precision=0.966]\n",
            "9/30 * Epoch (valid): 100% 80/80 [00:15<00:00,  5.02it/s, loss=1.171, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 12:13:14,990] \n",
            "9/30 * Epoch 9 (_base): lr=2.447e-05 | momentum=0.9000\n",
            "9/30 * Epoch 9 (train): epoch_mAP=0.9345 | epoch_macro_f1=0.8918 | epoch_precision=0.9340 | loss=0.8524 | mAP=0.9388 | macro_f1=0.8899 | precision=0.9333\n",
            "9/30 * Epoch 9 (valid): epoch_mAP=0.9360 | epoch_macro_f1=0.8949 | epoch_precision=0.8679 | loss=0.8586 | mAP=0.8908 | macro_f1=0.8521 | precision=0.7830\n",
            "10/30 * Epoch (train): 100% 317/317 [10:53<00:00,  2.06s/it, loss=1.098, mAP=0.854, macro_f1=0.806, precision=0.857]\n",
            "10/30 * Epoch (valid): 100% 80/80 [00:15<00:00,  5.03it/s, loss=1.166, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 12:24:27,795] \n",
            "10/30 * Epoch 10 (_base): lr=0.000e+00 | momentum=0.9000\n",
            "10/30 * Epoch 10 (train): epoch_mAP=0.9396 | epoch_macro_f1=0.8937 | epoch_precision=0.9348 | loss=0.8436 | mAP=0.9428 | macro_f1=0.8914 | precision=0.9341\n",
            "10/30 * Epoch 10 (valid): epoch_mAP=0.9370 | epoch_macro_f1=0.8994 | epoch_precision=0.8816 | loss=0.8469 | mAP=0.8896 | macro_f1=0.8632 | precision=0.7966\n",
            "11/30 * Epoch (train): 100% 317/317 [11:06<00:00,  2.10s/it, loss=1.066, mAP=0.866, macro_f1=0.831, precision=0.885]\n",
            "11/30 * Epoch (valid): 100% 80/80 [00:15<00:00,  5.03it/s, loss=1.174, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 12:35:53,936] \n",
            "11/30 * Epoch 11 (_base): lr=2.447e-05 | momentum=0.9000\n",
            "11/30 * Epoch 11 (train): epoch_mAP=0.9393 | epoch_macro_f1=0.8905 | epoch_precision=0.9338 | loss=0.8475 | mAP=0.9418 | macro_f1=0.8883 | precision=0.9338\n",
            "11/30 * Epoch 11 (valid): epoch_mAP=0.9370 | epoch_macro_f1=0.9007 | epoch_precision=0.8814 | loss=0.8475 | mAP=0.8900 | macro_f1=0.8642 | precision=0.7969\n",
            "12/30 * Epoch (train): 100% 317/317 [11:06<00:00,  2.10s/it, loss=1.049, mAP=0.887, macro_f1=0.792, precision=0.871]\n",
            "12/30 * Epoch (valid): 100% 80/80 [00:15<00:00,  5.05it/s, loss=1.130, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 12:47:19,797] \n",
            "12/30 * Epoch 12 (_base): lr=9.549e-05 | momentum=0.9000\n",
            "12/30 * Epoch 12 (train): epoch_mAP=0.9391 | epoch_macro_f1=0.8933 | epoch_precision=0.9329 | loss=0.8435 | mAP=0.9425 | macro_f1=0.8911 | precision=0.9322\n",
            "12/30 * Epoch 12 (valid): epoch_mAP=0.9362 | epoch_macro_f1=0.9027 | epoch_precision=0.8838 | loss=0.8469 | mAP=0.8876 | macro_f1=0.8653 | precision=0.7973\n",
            "13/30 * Epoch (train): 100% 317/317 [11:01<00:00,  2.09s/it, loss=0.763, mAP=0.983, macro_f1=0.903, precision=0.969]\n",
            "13/30 * Epoch (valid): 100% 80/80 [00:15<00:00,  5.02it/s, loss=1.293, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 12:58:40,627] \n",
            "13/30 * Epoch 13 (_base): lr=0.0002 | momentum=0.9000\n",
            "13/30 * Epoch 13 (train): epoch_mAP=0.9389 | epoch_macro_f1=0.8937 | epoch_precision=0.9340 | loss=0.8443 | mAP=0.9428 | macro_f1=0.8924 | precision=0.9339\n",
            "13/30 * Epoch 13 (valid): epoch_mAP=0.9373 | epoch_macro_f1=0.9040 | epoch_precision=0.8933 | loss=0.8436 | mAP=0.8908 | macro_f1=0.8658 | precision=0.8029\n",
            "14/30 * Epoch (train): 100% 317/317 [11:05<00:00,  2.10s/it, loss=0.665, mAP=0.997, macro_f1=0.977, precision=0.969]\n",
            "14/30 * Epoch (valid): 100% 80/80 [00:15<00:00,  5.04it/s, loss=1.267, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 13:10:07,175] \n",
            "14/30 * Epoch 14 (_base): lr=0.0003 | momentum=0.9000\n",
            "14/30 * Epoch 14 (train): epoch_mAP=0.9389 | epoch_macro_f1=0.8914 | epoch_precision=0.9308 | loss=0.8472 | mAP=0.9436 | macro_f1=0.8895 | precision=0.9311\n",
            "14/30 * Epoch 14 (valid): epoch_mAP=0.9368 | epoch_macro_f1=0.9071 | epoch_precision=0.9185 | loss=0.8466 | mAP=0.8884 | macro_f1=0.8709 | precision=0.8293\n",
            "15/30 * Epoch (train): 100% 317/317 [11:07<00:00,  2.10s/it, loss=1.087, mAP=0.848, macro_f1=0.798, precision=0.867]\n",
            "15/30 * Epoch (valid): 100% 80/80 [00:15<00:00,  5.01it/s, loss=1.326, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 13:21:34,589] \n",
            "15/30 * Epoch 15 (_base): lr=0.0005 | momentum=0.9000\n",
            "15/30 * Epoch 15 (train): epoch_mAP=0.9336 | epoch_macro_f1=0.8862 | epoch_precision=0.9281 | loss=0.8590 | mAP=0.9381 | macro_f1=0.8836 | precision=0.9280\n",
            "15/30 * Epoch 15 (valid): epoch_mAP=0.9329 | epoch_macro_f1=0.9002 | epoch_precision=0.8987 | loss=0.8681 | mAP=0.8816 | macro_f1=0.8627 | precision=0.8044\n",
            "16/30 * Epoch (train): 100% 317/317 [11:04<00:00,  2.10s/it, loss=0.860, mAP=0.923, macro_f1=0.885, precision=0.967]\n",
            "16/30 * Epoch (valid): 100% 80/80 [00:16<00:00,  4.97it/s, loss=1.702, mAP=0.500, macro_f1=0.391, precision=0.000e+00]\n",
            "[2021-02-12 13:32:58,530] \n",
            "16/30 * Epoch 16 (_base): lr=0.0007 | momentum=0.9000\n",
            "16/30 * Epoch 16 (train): epoch_mAP=0.9285 | epoch_macro_f1=0.8825 | epoch_precision=0.9245 | loss=0.8714 | mAP=0.9347 | macro_f1=0.8805 | precision=0.9240\n",
            "16/30 * Epoch 16 (valid): epoch_mAP=0.9342 | epoch_macro_f1=0.8992 | epoch_precision=0.9057 | loss=0.8547 | mAP=0.8824 | macro_f1=0.8465 | precision=0.8210\n",
            "17/30 * Epoch (train): 100% 317/317 [11:04<00:00,  2.10s/it, loss=0.761, mAP=0.981, macro_f1=0.885, precision=0.935]\n",
            "17/30 * Epoch (valid): 100% 80/80 [00:15<00:00,  5.02it/s, loss=2.079, mAP=0.500, macro_f1=0.391, precision=0.000e+00]\n",
            "[2021-02-12 13:44:22,454] \n",
            "17/30 * Epoch 17 (_base): lr=0.0008 | momentum=0.9000\n",
            "17/30 * Epoch 17 (train): epoch_mAP=0.9284 | epoch_macro_f1=0.8835 | epoch_precision=0.9273 | loss=0.8751 | mAP=0.9346 | macro_f1=0.8814 | precision=0.9266\n",
            "17/30 * Epoch 17 (valid): epoch_mAP=0.9318 | epoch_macro_f1=0.8913 | epoch_precision=0.8610 | loss=0.8835 | mAP=0.8827 | macro_f1=0.8463 | precision=0.7826\n",
            "18/30 * Epoch (train): 100% 317/317 [11:03<00:00,  2.09s/it, loss=0.768, mAP=0.977, macro_f1=0.947, precision=0.944]\n",
            "18/30 * Epoch (valid): 100% 80/80 [00:15<00:00,  5.02it/s, loss=1.864, mAP=0.500, macro_f1=0.391, precision=0.000e+00]\n",
            "[2021-02-12 13:55:45,663] \n",
            "18/30 * Epoch 18 (_base): lr=0.0009 | momentum=0.9000\n",
            "18/30 * Epoch 18 (train): epoch_mAP=0.9241 | epoch_macro_f1=0.8811 | epoch_precision=0.9229 | loss=0.8923 | mAP=0.9320 | macro_f1=0.8789 | precision=0.9233\n",
            "18/30 * Epoch 18 (valid): epoch_mAP=0.9305 | epoch_macro_f1=0.8923 | epoch_precision=0.8642 | loss=0.8674 | mAP=0.8772 | macro_f1=0.8463 | precision=0.7773\n",
            "19/30 * Epoch (train): 100% 317/317 [10:59<00:00,  2.08s/it, loss=0.851, mAP=0.978, macro_f1=0.895, precision=0.857]\n",
            "19/30 * Epoch (valid): 100% 80/80 [00:16<00:00,  4.98it/s, loss=1.286, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 14:07:04,972] \n",
            "19/30 * Epoch 19 (_base): lr=0.0010 | momentum=0.9000\n",
            "19/30 * Epoch 19 (train): epoch_mAP=0.9214 | epoch_macro_f1=0.8776 | epoch_precision=0.9207 | loss=0.8947 | mAP=0.9295 | macro_f1=0.8753 | precision=0.9208\n",
            "19/30 * Epoch 19 (valid): epoch_mAP=0.9121 | epoch_macro_f1=0.8595 | epoch_precision=0.7705 | loss=0.9464 | mAP=0.8662 | macro_f1=0.8064 | precision=0.6930\n",
            "20/30 * Epoch (train): 100% 317/317 [11:05<00:00,  2.10s/it, loss=0.991, mAP=0.873, macro_f1=0.863, precision=0.903]\n",
            "20/30 * Epoch (valid): 100% 80/80 [00:16<00:00,  5.00it/s, loss=0.923, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 14:18:29,947] \n",
            "20/30 * Epoch 20 (_base): lr=0.0010 | momentum=0.9000\n",
            "20/30 * Epoch 20 (train): epoch_mAP=0.9191 | epoch_macro_f1=0.8811 | epoch_precision=0.9238 | loss=0.8872 | mAP=0.9297 | macro_f1=0.8790 | precision=0.9235\n",
            "20/30 * Epoch 20 (valid): epoch_mAP=0.9280 | epoch_macro_f1=0.8969 | epoch_precision=0.8808 | loss=0.8581 | mAP=0.8800 | macro_f1=0.8601 | precision=0.7888\n",
            "21/30 * Epoch (train): 100% 317/317 [11:09<00:00,  2.11s/it, loss=0.842, mAP=0.932, macro_f1=0.859, precision=0.935]\n",
            "21/30 * Epoch (valid): 100% 80/80 [00:15<00:00,  5.01it/s, loss=1.082, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 14:29:58,703] \n",
            "21/30 * Epoch 21 (_base): lr=0.0010 | momentum=0.9000\n",
            "21/30 * Epoch 21 (train): epoch_mAP=0.9231 | epoch_macro_f1=0.8844 | epoch_precision=0.9268 | loss=0.8820 | mAP=0.9328 | macro_f1=0.8823 | precision=0.9265\n",
            "21/30 * Epoch 21 (valid): epoch_mAP=0.9347 | epoch_macro_f1=0.8600 | epoch_precision=0.7723 | loss=0.9366 | mAP=0.8825 | macro_f1=0.8114 | precision=0.6860\n",
            "22/30 * Epoch (train): 100% 317/317 [10:58<00:00,  2.08s/it, loss=0.933, mAP=0.969, macro_f1=0.812, precision=0.724]\n",
            "22/30 * Epoch (valid): 100% 80/80 [00:16<00:00,  4.99it/s, loss=1.303, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 14:41:16,660] \n",
            "22/30 * Epoch 22 (_base): lr=0.0009 | momentum=0.9000\n",
            "22/30 * Epoch 22 (train): epoch_mAP=0.9259 | epoch_macro_f1=0.8811 | epoch_precision=0.9222 | loss=0.8814 | mAP=0.9330 | macro_f1=0.8786 | precision=0.9225\n",
            "22/30 * Epoch 22 (valid): epoch_mAP=0.9325 | epoch_macro_f1=0.8883 | epoch_precision=0.8583 | loss=0.8671 | mAP=0.8855 | macro_f1=0.8504 | precision=0.7650\n",
            "23/30 * Epoch (train): 100% 317/317 [10:56<00:00,  2.07s/it, loss=0.824, mAP=0.927, macro_f1=0.915, precision=1.000]\n",
            "23/30 * Epoch (valid): 100% 80/80 [00:16<00:00,  4.99it/s, loss=1.234, mAP=0.500, macro_f1=0.391, precision=0.000e+00]\n",
            "[2021-02-12 14:52:33,026] \n",
            "23/30 * Epoch 23 (_base): lr=0.0008 | momentum=0.9000\n",
            "23/30 * Epoch 23 (train): epoch_mAP=0.9246 | epoch_macro_f1=0.8829 | epoch_precision=0.9255 | loss=0.8815 | mAP=0.9338 | macro_f1=0.8808 | precision=0.9254\n",
            "23/30 * Epoch 23 (valid): epoch_mAP=0.9330 | epoch_macro_f1=0.8804 | epoch_precision=0.8419 | loss=0.8757 | mAP=0.8846 | macro_f1=0.8323 | precision=0.7672\n",
            "24/30 * Epoch (train): 100% 317/317 [11:03<00:00,  2.09s/it, loss=1.096, mAP=0.867, macro_f1=0.790, precision=0.808]\n",
            "24/30 * Epoch (valid): 100% 80/80 [00:15<00:00,  5.02it/s, loss=0.813, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 15:03:55,958] \n",
            "24/30 * Epoch 24 (_base): lr=0.0007 | momentum=0.9000\n",
            "24/30 * Epoch 24 (train): epoch_mAP=0.9348 | epoch_macro_f1=0.8904 | epoch_precision=0.9284 | loss=0.8605 | mAP=0.9399 | macro_f1=0.8882 | precision=0.9275\n",
            "24/30 * Epoch 24 (valid): epoch_mAP=0.9299 | epoch_macro_f1=0.8817 | epoch_precision=0.8332 | loss=0.8825 | mAP=0.8825 | macro_f1=0.8237 | precision=0.7477\n",
            "25/30 * Epoch (train): 100% 317/317 [10:53<00:00,  2.06s/it, loss=0.907, mAP=0.919, macro_f1=0.869, precision=0.963]\n",
            "25/30 * Epoch (valid): 100% 80/80 [00:16<00:00,  4.98it/s, loss=0.994, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 15:15:08,959] \n",
            "25/30 * Epoch 25 (_base): lr=0.0005 | momentum=0.9000\n",
            "25/30 * Epoch 25 (train): epoch_mAP=0.9384 | epoch_macro_f1=0.8967 | epoch_precision=0.9378 | loss=0.8461 | mAP=0.9431 | macro_f1=0.8944 | precision=0.9377\n",
            "25/30 * Epoch 25 (valid): epoch_mAP=0.9337 | epoch_macro_f1=0.9043 | epoch_precision=0.9117 | loss=0.8509 | mAP=0.8827 | macro_f1=0.8735 | precision=0.8326\n",
            "26/30 * Epoch (train): 100% 317/317 [11:07<00:00,  2.11s/it, loss=0.814, mAP=0.966, macro_f1=0.917, precision=0.955]\n",
            "26/30 * Epoch (valid): 100% 80/80 [00:15<00:00,  5.02it/s, loss=0.763, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 15:26:36,069] \n",
            "26/30 * Epoch 26 (_base): lr=0.0003 | momentum=0.9000\n",
            "26/30 * Epoch 26 (train): epoch_mAP=0.9431 | epoch_macro_f1=0.9002 | epoch_precision=0.9381 | loss=0.8284 | mAP=0.9462 | macro_f1=0.8985 | precision=0.9379\n",
            "26/30 * Epoch 26 (valid): epoch_mAP=0.9320 | epoch_macro_f1=0.8989 | epoch_precision=0.8851 | loss=0.8528 | mAP=0.8808 | macro_f1=0.8599 | precision=0.7928\n",
            "27/30 * Epoch (train): 100% 317/317 [11:03<00:00,  2.09s/it, loss=0.882, mAP=0.944, macro_f1=0.916, precision=0.960]\n",
            "27/30 * Epoch (valid): 100% 80/80 [00:16<00:00,  5.00it/s, loss=0.815, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 15:37:59,325] \n",
            "27/30 * Epoch 27 (_base): lr=0.0002 | momentum=0.9000\n",
            "27/30 * Epoch 27 (train): epoch_mAP=0.9516 | epoch_macro_f1=0.9057 | epoch_precision=0.9425 | loss=0.8089 | mAP=0.9547 | macro_f1=0.9039 | precision=0.9422\n",
            "27/30 * Epoch 27 (valid): epoch_mAP=0.9259 | epoch_macro_f1=0.8922 | epoch_precision=0.8705 | loss=0.8772 | mAP=0.8775 | macro_f1=0.8428 | precision=0.7837\n",
            "28/30 * Epoch (train): 100% 317/317 [11:10<00:00,  2.12s/it, loss=0.649, mAP=0.986, macro_f1=0.974, precision=1.000]\n",
            "28/30 * Epoch (valid): 100% 80/80 [00:15<00:00,  5.04it/s, loss=0.850, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 15:49:29,379] \n",
            "28/30 * Epoch 28 (_base): lr=9.549e-05 | momentum=0.9000\n",
            "28/30 * Epoch 28 (train): epoch_mAP=0.9592 | epoch_macro_f1=0.9109 | epoch_precision=0.9442 | loss=0.7852 | mAP=0.9610 | macro_f1=0.9092 | precision=0.9438\n",
            "28/30 * Epoch 28 (valid): epoch_mAP=0.9254 | epoch_macro_f1=0.8934 | epoch_precision=0.8866 | loss=0.8776 | mAP=0.8764 | macro_f1=0.8498 | precision=0.7976\n",
            "29/30 * Epoch (train): 100% 317/317 [10:56<00:00,  2.07s/it, loss=0.975, mAP=0.926, macro_f1=0.831, precision=0.821]\n",
            "29/30 * Epoch (valid): 100% 80/80 [00:15<00:00,  5.02it/s, loss=0.671, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 16:00:45,404] \n",
            "29/30 * Epoch 29 (_base): lr=2.447e-05 | momentum=0.9000\n",
            "29/30 * Epoch 29 (train): epoch_mAP=0.9628 | epoch_macro_f1=0.9144 | epoch_precision=0.9465 | loss=0.7726 | mAP=0.9655 | macro_f1=0.9130 | precision=0.9458\n",
            "29/30 * Epoch 29 (valid): epoch_mAP=0.9258 | epoch_macro_f1=0.8918 | epoch_precision=0.8721 | loss=0.8963 | mAP=0.8744 | macro_f1=0.8478 | precision=0.7831\n",
            "30/30 * Epoch (train): 100% 317/317 [11:00<00:00,  2.08s/it, loss=0.791, mAP=0.968, macro_f1=0.869, precision=0.929]\n",
            "30/30 * Epoch (valid): 100% 80/80 [00:16<00:00,  5.00it/s, loss=0.711, mAP=0.500, macro_f1=1.000, precision=0.000e+00]\n",
            "[2021-02-12 16:12:05,309] \n",
            "30/30 * Epoch 30 (_base): lr=0.000e+00 | momentum=0.9000\n",
            "30/30 * Epoch 30 (train): epoch_mAP=0.9652 | epoch_macro_f1=0.9177 | epoch_precision=0.9501 | loss=0.7665 | mAP=0.9665 | macro_f1=0.9163 | precision=0.9495\n",
            "30/30 * Epoch 30 (valid): epoch_mAP=0.9251 | epoch_macro_f1=0.8847 | epoch_precision=0.8389 | loss=0.9067 | mAP=0.8774 | macro_f1=0.8278 | precision=0.7526\n",
            "Top best models:\n",
            "train\\logs\\sp3-3\\checkpoints/train.14.pth\t0.9185\n",
            "train\\logs\\sp3-3\\checkpoints/train.25.pth\t0.9117\n",
            "train\\logs\\sp3-3\\checkpoints/train.16.pth\t0.9057\n"
          ]
        }
      ],
      "source": [
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "runner = SupervisedRunner(\n",
        "    device=device,\n",
        "    input_key=\"waveform\",\n",
        "    input_target_key=\"targets\")\n",
        "\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=loss,\n",
        "    loaders=loaders,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    num_epochs=EPOCHS,\n",
        "    verbose=True,\n",
        "    logdir=LOG_DIR,\n",
        "    callbacks=callbacks,\n",
        "    main_metric=\"epoch_precision\",\n",
        "    minimize_metric=False,\n",
        "    # fp16=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8qk2nDSQzNe"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}